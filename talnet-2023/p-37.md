Methodology
About this study
From a research perspective, it was important to 
eliminate any perception of sample bias and ensure 
high data quality. We eliminated sample bias by 
sourcing 80% of our usable sample from a 3rd party 
panel provider and the remaining 20% from Linux 
Foundation membership. We addressed data quality 
through extensive pre-screening and screening 
criteria to ensure that respondents had sufficient 
familiarity and professional experience to answer 
questions accurately on behalf of the organization 
they worked for.


The design of the worldwide survey conducted by 
Linux Foundation Training & Certification and Linux 
Foundation Research was to gather insights on the 
current trends and challenges related to technical 
talent hiring and management. We conducted the 
survey from February 23 through March 10, 2023, 
and we promoted it through various channels, 
including social media, the Linux Foundation and 
Linux.com websites, and the Linux Foundation 
Newsletter.


To ensure the accuracy and reliability of the survey 
data, we also utilized a third-party panel provider 
to obtain 80% of the respondents. These respon-
dents received nominal compensation for their 
participation in the study. It is important to note 
that the screening of the survey participants was 
dependent on their employment status, and only 


those who were employed full-time were part of the 
study. Individuals who were not affiliated with any 
organization or were unable to talk about their orga-
nization's hiring plans and practices were not part of 
the survey.


In total, the survey received 437 responses. We 
removed 19 responses, as they could not provide 
answers to questions about the 2023 hiring trends. 
The final sample size used in most of the report was 
418 respondents. The margin of error for the survey 
data is + / - 4.9% at a 95% confidence level, indicating 
that the survey results are statistically significant 
and representative of the broader population.


For more details about the screening criteria used 
and access to the survey dataset, see http://www.
data.world/thelinuxfoundation. 


How missing data is handled
Although the requirement is for respondents to 
answer nearly all questions in the survey (the only 
exceptions are the open-ended questions), there 
are times when a respondent is unable to answer 
a question because it is outside the scope of their 
role or experience. For this reason, we frequently 
add a DKNS response to the list of responses for 
a question. However, this creates a conundrum 
regarding what to do with DKNS responses.


One approach is to treat it just like any other 
response. In this way, report readers can see the 
percentage of respondents that answered DKNS. 
The advantage of this approach is that it reports 
back the exact distribution of the data collected. The 
challenge with this approach is that it distorts the 
distribution of valid responsesâ€”those responses 
where respondents could answer the question.


Some of the analyses in this report excluded the 
DKNS. This can occur when the data missing can 
either be classified as missing at random (MAR) or 
missing completely at random (MCAR). Excluding 
DKNS data from a question does not change the 
distribution of data (counts) for the other responses, 
but it does change the size of the denominator used 
to calculate the percentage of responses across 
the remaining responses. This has the effect of 
proportionally increasing the percent values of the 
remaining responses relative to the number of DKNS 
responses. The number of valid cases is adjusted 
accordingly. Where we have elected to exclude 
DKNS data, a careful examination of the footnote for 
the figure will enable the reader to determine the 
number of DKNS responses based on the difference 
between the sample size (DKNS inclusive) and valid 
cases (DKNS excluded).


Finally, percentage values in this report may not add 
up to exactly 100% due to rounding.


